{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "informative-syndicate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T22:13:38.715788Z",
     "iopub.status.busy": "2021-03-05T22:13:38.714814Z",
     "iopub.status.idle": "2021-03-05T22:13:38.719772Z",
     "shell.execute_reply": "2021-03-05T22:13:38.719772Z",
     "shell.execute_reply.started": "2021-03-05T22:13:38.715788Z"
    }
   },
   "outputs": [],
   "source": [
    "# import dependencies \n",
    "import requests \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "# psycopg2 works in tandem with sqlalchemy \n",
    "import psycopg2\n",
    "# import custom module with database parameters \n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "present-boards",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T22:13:39.660474Z",
     "iopub.status.busy": "2021-03-05T22:13:39.659477Z",
     "iopub.status.idle": "2021-03-05T22:13:39.679424Z",
     "shell.execute_reply": "2021-03-05T22:13:39.678431Z",
     "shell.execute_reply.started": "2021-03-05T22:13:39.660474Z"
    }
   },
   "outputs": [],
   "source": [
    "# etl pipelien function \n",
    "#--extraction---\n",
    "def extract_transform_load(api_endpoint,parameters=None):\n",
    "    \n",
    "    data_request = requests.get(url=api_endpoint, params=parameters)\n",
    "    \n",
    "    data_df = pd.DataFrame.from_records(data_request.json())\n",
    "    \n",
    "    report_df = pd.DataFrame({'Rows Retrieved':{0:len(data_df)}})\n",
    "    \n",
    "    \n",
    " #--cleaning--\n",
    "    #drop columns \n",
    "    drop_columns = ['location', 'x_coordinate', 'y_coordinate',\n",
    "                    'occ_date', 'occ_time','rep_date','rep_time','category_description', 'address',\n",
    "                    'ucr_category','census_tract','sector','pra','council_district']\n",
    "    \n",
    "    data_df.drop(drop_columns, axis=1, inplace=True)\n",
    "    \n",
    "    #drop null values \n",
    "    # drop rows with missing data\n",
    "    column_checklist = ['incident_report_number', 'crime_type', 'ucr_code', 'family_violence',\n",
    "                       'occ_date_time', 'rep_date_time', 'location_type', 'zip_code',\n",
    "                         'latitude', 'longitude','district']\n",
    "    # track how many row with missing data is being dropped \n",
    "    row_counter = 0\n",
    "    for i in column_checklist:\n",
    "        if data_df[i].isnull().sum() > 0:\n",
    "            row_counter += data_df[i].isnull().sum()\n",
    "            data_df.drop(data_df[data_df[i].isnull()].index, inplace=True)\n",
    "    \n",
    "    report_df['Rows Dropped'] = row_counter\n",
    "                 \n",
    "    #fill in clearance data with placeholders \n",
    "    data_df['clearance_date'].fillna(value='0000-00-00T00:00:00.000',inplace=True)\n",
    "    data_df['clearance_status'].fillna(value='N', inplace=True)\n",
    "        \n",
    "    #clean and convert datetiem columns \n",
    "    data_df['occ_date_time'] = data_df['occ_date_time'].apply(lambda x: x.replace('T',' '))\n",
    "    data_df['rep_date_time'] = data_df['rep_date_time'].apply(lambda x: x.replace('T',' '))\n",
    "    data_df['clearance_date'] = data_df['clearance_date'].apply(lambda x: x.replace('T',' '))\n",
    "    \n",
    "    #rename columns \n",
    "    column_names = {'ucr_code':'offense_code','occ_date_time':'occurred_date',\n",
    "                    'rep_date_time':'reported_date','crime_type':'offense_type'}\n",
    "    \n",
    "    data_df.rename(columns=column_names, inplace=True)\n",
    "    \n",
    "    print(report_df)\n",
    "    \n",
    "   \n",
    " #--transform--\n",
    "    #offense_type_table \n",
    "    offense_df = data_df[['offense_code','offense_type']].copy()\n",
    "    offense_df.drop_duplicates(subset='offense_code',inplace=True)\n",
    "    \n",
    "    #create incident_location_table \n",
    "    # double brackets needed to create series, works like \"to_frame\" but is inplace \n",
    "    location_df = data_df[['location_type']].copy()\n",
    "    location_df.drop_duplicates(inplace=True)\n",
    "    location_df['location_code'] = np.arange(len(location_df))\n",
    "    \n",
    "    #create mapping for location_code column\n",
    "    location_map_df = location_df.copy(deep=True)\n",
    "    location_map_df.set_index('location_type', inplace=True) \n",
    "    location_mapper = location_map_df.to_dict()['location_code']\n",
    "        \n",
    "    # rearrange location_df\n",
    "    location_df = location_df[['location_code','location_type']]\n",
    "    \n",
    "    #crime_incidents_table \n",
    "    #create encoded location_code column\n",
    "    crime_incident_df = data_df.copy()\n",
    "    crime_incident_df['location_code'] = crime_incident_df['location_type'] \\\n",
    "        .apply(lambda x: location_mapper[x])\n",
    "    \n",
    "    #drop repetitive offense and location columns \n",
    "    drop_column_2 = ['offense_type','location_type']\n",
    "    crime_incident_df.drop(drop_column_2,axis=1, inplace=True)\n",
    "    \n",
    "#--load--\n",
    "    #setup database connection \n",
    "    database = f\"postgres://{config.db_user}:{config.db_password}@localhost:5432/austin_crime\"\n",
    "    engine = create_engine(database)\n",
    "    \n",
    "    #load crime table \n",
    "    print('loading crime_incident table')\n",
    "    start_time = time.time()\n",
    "    crime_test_df.to_sql(name='crime_incidents',index=False, con=engine, if_exists='replace', chunksize=100000)\n",
    "    print(f'{time.time() - start_time} seconds to load crime table \\n')                                    \n",
    "    \n",
    "    #load location table \n",
    "    print('loading incident_location table \\n')\n",
    "    location_test_df.to_sql(name='incident_location', index=False, con=engine, if_exists='replace')\n",
    "    \n",
    "    #load offense table \n",
    "    print('loading offense_type table')\n",
    "    offense_test_df.to_sql(name='offense_type', index=False, con=engine, if_exists='replace')\n",
    "    \n",
    "    \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "french-nursing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T22:13:41.166559Z",
     "iopub.status.busy": "2021-03-05T22:13:41.166559Z",
     "iopub.status.idle": "2021-03-05T22:13:41.170548Z",
     "shell.execute_reply": "2021-03-05T22:13:41.170548Z",
     "shell.execute_reply.started": "2021-03-05T22:13:41.166559Z"
    }
   },
   "outputs": [],
   "source": [
    "# api endpoint with custom date filter for data beween 2018 - 2020\n",
    "url = \"https://data.austintexas.gov/resource/fdj4-gpfu.json?$limit=1000&$where=occ_date between '2018-01-01T00:00:00.000' and '2020-12-31T00:00:00.000'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "great-plain",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T22:13:42.142949Z",
     "iopub.status.busy": "2021-03-05T22:13:42.142949Z",
     "iopub.status.idle": "2021-03-05T22:13:42.909097Z",
     "shell.execute_reply": "2021-03-05T22:13:42.908100Z",
     "shell.execute_reply.started": "2021-03-05T22:13:42.142949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rows Retrieved  Rows Dropped\n",
      "0            1000            28\n",
      "loading crime_incident table\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'crime_test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a8c3ffcd4344>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mextract_transform_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-217e166a9f39>\u001b[0m in \u001b[0;36mextract_transform_load\u001b[1;34m(api_endpoint, parameters)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loading crime_incident table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mcrime_test_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'crime_incidents'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{time.time() - start_time} seconds to load crime table \\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'crime_test_df' is not defined"
     ]
    }
   ],
   "source": [
    "extract_transform_load(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-hearing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
